{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = pd.read_csv('scipop_allfeat.csv', encoding = 'utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = texts.drop(['diff words persent'] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>avg sent length-chars</th>\n",
       "      <th>common words</th>\n",
       "      <th>talk words</th>\n",
       "      <th>nouns</th>\n",
       "      <th>adjs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228</td>\n",
       "      <td>215.29</td>\n",
       "      <td>79.81</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>239</td>\n",
       "      <td>174.38</td>\n",
       "      <td>74.15</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>254</td>\n",
       "      <td>160.60</td>\n",
       "      <td>71.54</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203</td>\n",
       "      <td>89.59</td>\n",
       "      <td>46.27</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>128.80</td>\n",
       "      <td>66.33</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>285</td>\n",
       "      <td>138.86</td>\n",
       "      <td>70.11</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>254</td>\n",
       "      <td>251.00</td>\n",
       "      <td>85.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>244</td>\n",
       "      <td>180.67</td>\n",
       "      <td>76.27</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>207</td>\n",
       "      <td>186.00</td>\n",
       "      <td>69.85</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>257.62</td>\n",
       "      <td>68.80</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>426</td>\n",
       "      <td>201.07</td>\n",
       "      <td>68.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>211</td>\n",
       "      <td>165.67</td>\n",
       "      <td>75.60</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93</td>\n",
       "      <td>96.88</td>\n",
       "      <td>51.09</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>322</td>\n",
       "      <td>159.57</td>\n",
       "      <td>77.53</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>335</td>\n",
       "      <td>119.56</td>\n",
       "      <td>82.93</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>188</td>\n",
       "      <td>171.00</td>\n",
       "      <td>54.30</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>377</td>\n",
       "      <td>93.24</td>\n",
       "      <td>75.80</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>218</td>\n",
       "      <td>104.12</td>\n",
       "      <td>66.36</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>311</td>\n",
       "      <td>217.20</td>\n",
       "      <td>77.63</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>368</td>\n",
       "      <td>134.06</td>\n",
       "      <td>66.48</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>440</td>\n",
       "      <td>110.52</td>\n",
       "      <td>69.52</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>235</td>\n",
       "      <td>135.27</td>\n",
       "      <td>72.37</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>538</td>\n",
       "      <td>133.56</td>\n",
       "      <td>80.23</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>376</td>\n",
       "      <td>162.75</td>\n",
       "      <td>79.51</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>259</td>\n",
       "      <td>105.67</td>\n",
       "      <td>74.38</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>238</td>\n",
       "      <td>149.55</td>\n",
       "      <td>64.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>230</td>\n",
       "      <td>127.69</td>\n",
       "      <td>75.55</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>365</td>\n",
       "      <td>171.31</td>\n",
       "      <td>70.08</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>193</td>\n",
       "      <td>113.62</td>\n",
       "      <td>61.34</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>292</td>\n",
       "      <td>158.92</td>\n",
       "      <td>78.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>174</td>\n",
       "      <td>117.70</td>\n",
       "      <td>69.82</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>138</td>\n",
       "      <td>157.83</td>\n",
       "      <td>58.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>188</td>\n",
       "      <td>117.20</td>\n",
       "      <td>83.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>213</td>\n",
       "      <td>134.80</td>\n",
       "      <td>63.94</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>292</td>\n",
       "      <td>112.88</td>\n",
       "      <td>71.79</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>246</td>\n",
       "      <td>156.18</td>\n",
       "      <td>69.39</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>303</td>\n",
       "      <td>152.29</td>\n",
       "      <td>69.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>276</td>\n",
       "      <td>142.42</td>\n",
       "      <td>69.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>246</td>\n",
       "      <td>208.56</td>\n",
       "      <td>72.69</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>447</td>\n",
       "      <td>145.45</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>322</td>\n",
       "      <td>99.79</td>\n",
       "      <td>66.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>260</td>\n",
       "      <td>159.18</td>\n",
       "      <td>72.91</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>9743</td>\n",
       "      <td>84.69</td>\n",
       "      <td>84.90</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>18177</td>\n",
       "      <td>82.52</td>\n",
       "      <td>85.24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>6332</td>\n",
       "      <td>49.41</td>\n",
       "      <td>81.30</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>15043</td>\n",
       "      <td>88.52</td>\n",
       "      <td>77.86</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>5545</td>\n",
       "      <td>115.44</td>\n",
       "      <td>83.82</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>6871</td>\n",
       "      <td>102.65</td>\n",
       "      <td>80.45</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>8333</td>\n",
       "      <td>122.46</td>\n",
       "      <td>83.81</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>16309</td>\n",
       "      <td>81.38</td>\n",
       "      <td>82.42</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>9017</td>\n",
       "      <td>94.57</td>\n",
       "      <td>85.73</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>333</td>\n",
       "      <td>92.00</td>\n",
       "      <td>78.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>165</td>\n",
       "      <td>129.25</td>\n",
       "      <td>78.40</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>173</td>\n",
       "      <td>149.43</td>\n",
       "      <td>76.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>232</td>\n",
       "      <td>101.27</td>\n",
       "      <td>73.42</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>337</td>\n",
       "      <td>124.88</td>\n",
       "      <td>65.54</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>768</td>\n",
       "      <td>83.69</td>\n",
       "      <td>86.90</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>379</td>\n",
       "      <td>121.25</td>\n",
       "      <td>77.66</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>165</td>\n",
       "      <td>102.60</td>\n",
       "      <td>64.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>510</td>\n",
       "      <td>105.17</td>\n",
       "      <td>73.57</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>485 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     words  avg sent length-chars  common words  talk words  nouns  adjs\n",
       "0      228                 215.29         79.81        0.17   0.34  0.16\n",
       "1      239                 174.38         74.15        0.26   0.34  0.18\n",
       "2      254                 160.60         71.54        0.23   0.38  0.15\n",
       "3      203                  89.59         46.27        0.22   0.40  0.12\n",
       "4      304                 128.80         66.33        0.23   0.40  0.13\n",
       "5      285                 138.86         70.11        0.21   0.43  0.13\n",
       "6      254                 251.00         85.25        0.22   0.36  0.17\n",
       "7      244                 180.67         76.27        0.19   0.40  0.15\n",
       "8      207                 186.00         69.85        0.20   0.33  0.17\n",
       "9      256                 257.62         68.80        0.16   0.44  0.24\n",
       "10     426                 201.07         68.20        0.15   0.42  0.16\n",
       "11     211                 165.67         75.60        0.21   0.37  0.12\n",
       "12      93                  96.88         51.09        0.21   0.51  0.13\n",
       "13     322                 159.57         77.53        0.11   0.39  0.14\n",
       "14     335                 119.56         82.93        0.21   0.37  0.19\n",
       "15     188                 171.00         54.30        0.18   0.46  0.13\n",
       "16     377                  93.24         75.80        0.23   0.32  0.18\n",
       "17     218                 104.12         66.36        0.22   0.43  0.16\n",
       "18     311                 217.20         77.63        0.15   0.38  0.18\n",
       "19     368                 134.06         66.48        0.18   0.40  0.16\n",
       "20     440                 110.52         69.52        0.24   0.33  0.16\n",
       "21     235                 135.27         72.37        0.15   0.42  0.16\n",
       "22     538                 133.56         80.23        0.20   0.36  0.18\n",
       "23     376                 162.75         79.51        0.20   0.39  0.15\n",
       "24     259                 105.67         74.38        0.27   0.44  0.17\n",
       "25     238                 149.55         64.38        0.20   0.40  0.18\n",
       "26     230                 127.69         75.55        0.23   0.38  0.21\n",
       "27     365                 171.31         70.08        0.24   0.30  0.19\n",
       "28     193                 113.62         61.34        0.19   0.44  0.11\n",
       "29     292                 158.92         78.69        0.19   0.36  0.15\n",
       "..     ...                    ...           ...         ...    ...   ...\n",
       "455    174                 117.70         69.82        0.12   0.38  0.12\n",
       "456    138                 157.83         58.52        0.13   0.30  0.16\n",
       "457    188                 117.20         83.07        0.17   0.42  0.15\n",
       "458    213                 134.80         63.94        0.16   0.33  0.17\n",
       "459    292                 112.88         71.79        0.15   0.38  0.19\n",
       "460    246                 156.18         69.39        0.15   0.34  0.15\n",
       "461    303                 152.29         69.05        0.12   0.32  0.15\n",
       "462    276                 142.42         69.26        0.17   0.41  0.13\n",
       "463    246                 208.56         72.69        0.18   0.33  0.17\n",
       "464    447                 145.45         72.99        0.11   0.34  0.13\n",
       "465    322                  99.79         66.03        0.18   0.27  0.18\n",
       "466    260                 159.18         72.91        0.17   0.43  0.14\n",
       "467   9743                  84.69         84.90        0.26   0.39  0.25\n",
       "468  18177                  82.52         85.24        0.25   0.46  0.18\n",
       "469   6332                  49.41         81.30        0.29   0.36  0.19\n",
       "470  15043                  88.52         77.86        0.22   0.30  0.18\n",
       "471   5545                 115.44         83.82        0.26   0.34  0.15\n",
       "472   6871                 102.65         80.45        0.21   0.28  0.19\n",
       "473   8333                 122.46         83.81        0.25   0.35  0.18\n",
       "474  16309                  81.38         82.42        0.29   0.36  0.22\n",
       "475   9017                  94.57         85.73        0.27   0.34  0.19\n",
       "476    333                  92.00         78.22        0.22   0.34  0.18\n",
       "477    165                 129.25         78.40        0.23   0.30  0.19\n",
       "478    173                 149.43         76.02        0.17   0.27  0.16\n",
       "479    232                 101.27         73.42        0.16   0.31  0.16\n",
       "480    337                 124.88         65.54        0.16   0.30  0.20\n",
       "481    768                  83.69         86.90        0.21   0.31  0.15\n",
       "482    379                 121.25         77.66        0.18   0.24  0.18\n",
       "483    165                 102.60         64.07        0.20   0.26  0.16\n",
       "484    510                 105.17         73.57        0.22   0.31  0.18\n",
       "\n",
       "[485 rows x 6 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = texts['category'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(texts.values, y, test_size=0.3,\n",
    "random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_fit = scaler.fit(X_train)\n",
    "x_transform = scaler.transform(X_train)\n",
    "x_fit_test = scaler.fit(X_test)\n",
    "x_transform_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.43056715, -0.06474255, -1.14104444,  0.56314844,  0.7632449 ,\n",
       "        -1.1223227 ],\n",
       "       [-0.01062461, -1.04781795, -1.52593951, -1.42087684,  0.7632449 ,\n",
       "         0.7246064 ],\n",
       "       [-0.14526558, -0.03423873,  0.72590211,  0.56314844, -0.17489876,\n",
       "         0.7246064 ],\n",
       "       ...,\n",
       "       [ 0.58019374,  0.53142775,  0.63002077, -0.09819332, -0.73778496,\n",
       "        -1.1223227 ],\n",
       "       [-0.47862597,  0.03932931,  0.16431143,  0.7835957 ,  0.01272997,\n",
       "        -0.01416524],\n",
       "       [-0.46527629,  0.06983314, -1.46841071, -0.97998234,  0.7632449 ,\n",
       "        -1.49170852]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.51      0.71      0.59        56\n",
      "          2       0.36      0.43      0.40        46\n",
      "          3       0.25      0.07      0.11        44\n",
      "\n",
      "avg / total       0.38      0.43      0.38       146\n",
      "\n",
      "[[40 13  3]\n",
      " [20 20  6]\n",
      " [19 22  3]]\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC()\n",
    "model_svc.fit(x_transform, y_train)\n",
    "print(model_svc)\n",
    "\n",
    "expected_svc = y_test\n",
    "predicted_svc = model_svc.predict(x_transform_test)\n",
    "\n",
    "print(metrics.classification_report(expected_svc, predicted_svc))\n",
    "print(metrics.confusion_matrix(expected_svc, predicted_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='ball_tree', leaf_size=100, metric='minkowski',\n",
      "           metric_params=None, n_jobs=5, n_neighbors=10, p=10,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.43      0.55      0.48        56\n",
      "          2       0.37      0.41      0.39        46\n",
      "          3       0.39      0.20      0.27        44\n",
      "\n",
      "avg / total       0.40      0.40      0.39       146\n",
      "\n",
      "[[31 18  7]\n",
      " [20 19  7]\n",
      " [21 14  9]]\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=10, algorithm='ball_tree', leaf_size = 100, p=10, n_jobs = 5)\n",
    "model.fit(x_transform, y_train)\n",
    "print(model)\n",
    "\n",
    "expected = y_test\n",
    "predicted = model.predict(x_transform_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC().fit(x_transform, y_train)\n",
    "predicted = clf.predict(x_transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.4041\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(predicted, y_test)\n",
    "#micro_f1 = f1_score(predicted, y_test, average = 'micro')\n",
    "#micro_p = precision_score(predicted, y_test, average = 'micro')\n",
    "#micro_r = recall_score(predicted, y_test, average = 'micro')\n",
    "#macro_f1 = f1_score(predicted, y_test, average = 'macro')\n",
    "#macro_p = precision_score(predicted, y_test, average = 'macro')\n",
    "#macro_r = recall_score(predicted, y_test, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "#print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "#print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier().fit(x_transform, y_train)\n",
    "predicted = clf.predict(x_transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.4041\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(predicted, y_test)\n",
    "#micro_f1 = f1_score(predicted, y_test, average = 'micro')\n",
    "#micro_p = precision_score(predicted, y_test, average = 'micro')\n",
    "#micro_r = recall_score(predicted, y_test, average = 'micro')\n",
    "#macro_f1 = f1_score(predicted, y_test, average = 'macro')\n",
    "#macro_p = precision_score(predicted, y_test, average = 'macro')\n",
    "#macro_r = recall_score(predicted, y_test, average = 'macro')\n",
    "print('acc={0:1.4f}'.format(acc))\n",
    "#print('micro F1={0:1.4f}, micro P={1:1.4f}, micro R={2:1.4f}'.format(micro_f1, micro_p, micro_r))\n",
    "#print('macro F1={0:1.4f}, macro P={1:1.4f}, macro R={2:1.4f}\\n'.format(macro_f1, macro_p, macro_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
