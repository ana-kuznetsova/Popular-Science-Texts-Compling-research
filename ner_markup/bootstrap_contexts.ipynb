{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/home/alapidus/NIS/articles_with_meta.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slurp_lists(path):\n",
    "    with open(path, 'r') as fo:\n",
    "        return fo.readlines()\n",
    "\n",
    "def spit(path, text):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as file_object:\n",
    "        return file_object.write(text)\n",
    "    \n",
    "names_extracted = slurp_lists('/home/alapidus/NIS/ner_data/ner_corpora_extracted.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark talks and lectures\n",
    "\n",
    "* Keep file name (path)\n",
    "* Source: postnauka, polit.ru (lectures)\n",
    "* Genre: talks, lectures (from polit.ru)\n",
    "* Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filenames(roots):\n",
    "    fnames = []\n",
    "    for root in roots:\n",
    "        if '\\\\' in root:\n",
    "            index = root.rfind('\\\\')\n",
    "            fname = root[index+1:]\n",
    "            fnames.append(fname)\n",
    "        else:\n",
    "            index = root.rfind('/')\n",
    "            fname = root[index+1:]\n",
    "            fnames.append(fname)\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, source):\n",
    "    if source == 'https://postnauka.ru':\n",
    "        subset = df.loc[df['source'] == 'https://postnauka.ru']\n",
    "        roots = subset.path\n",
    "        fnames = generate_filenames(roots)\n",
    "        texts = subset.text\n",
    "    elif source == 'polit.ru/lectures':\n",
    "        subset = df.loc[df['source'] == 'polit.ru/lectures']\n",
    "        roots = subset.path\n",
    "        fnames = generate_filenames(roots)\n",
    "        texts = subset.text\n",
    "    return fnames, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lec_names, lec_scripts = get_texts(meta, 'polit.ru/lectures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_names, post_texts = get_texts(meta, 'https://postnauka.ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_extracted = [name.rstrip() for name in names_extracted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_names2(text, names:list):\n",
    "    search_pattern = '|'.join(names)\n",
    "    #print(search_pattern)\n",
    "    replace_pattern = r'&\\g<0>!&'\n",
    "    return re.sub(search_pattern, replace_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = ' '.join(re.findall(r'[А-ЯЁа-яё\\-\\.]+', text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_names(text):\n",
    "    text_split = preprocess(text).split()\n",
    "    for word in text_split:\n",
    "        if names_processor.extract_keywords(word) == [word]:\n",
    "            index = text_split.index(word)\n",
    "            text_split[index] = '&' + word + '!&'\n",
    "    return ' '.join(text_split)\n",
    "\n",
    "\n",
    "def tag_texts(texts:list, names:list):\n",
    "    texts_marked = []\n",
    "    names_processor = KeywordProcessor()\n",
    "    names_processor.add_keywords_from_list(names)\n",
    "    for text in tqdm(texts):\n",
    "        text_marked = tag_names(text)\n",
    "        texts_marked.append(text_marked)\n",
    "    return texts_marked        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_sub = [name.replace('.', '\\.') if '.' in name else name for name in names_extracted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 562/562 [00:52<00:00, 10.69it/s]\n"
     ]
    }
   ],
   "source": [
    "tagged_lectures = tag_texts(lec_scripts, names_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4054/4054 [00:57<00:00, 71.03it/s] \n"
     ]
    }
   ],
   "source": [
    "tagged_postn = tag_texts(post_texts, names_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '/home/alapidus/NIS/ner_data/marked_talks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_dir(fnames, texts, path):\n",
    "    for text, fname in tqdm(zip(texts, fnames)):\n",
    "        spit(path+fname, text)\n",
    "    print('Over!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "562it [00:00, 1823.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_dir(lec_names, tagged_lectures, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4054it [00:00, 6631.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_dir(post_names, tagged_postn, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET NE CONTEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    def __init__(self, left_context: str, right_context: str,\n",
    "                 left_word: str, word: str):\n",
    "        self.left_context = left_context\n",
    "        self.right_context = right_context\n",
    "        self.left_word = left_word\n",
    "        self.word = word\n",
    "    def __repr__(self):\n",
    "        return 'Context(\"%s\", \"%s\", \"%s\", \"%s\")' % (\n",
    "            self.left_context, self.right_context,\n",
    "            self.left_word, self.word)\n",
    "\n",
    "def tokenize_text(text: str) -> [str]:\n",
    "    return re.findall(r'&?[\\w.\\'-]+!?&?', text, flags=re.UNICODE)\n",
    "\n",
    "def cleanup_tag(text: str) -> str:\n",
    "    return text.replace('&', '').replace('!', '')\n",
    "\n",
    "def get_span(tokens: [str], start: int, end: int) -> str:\n",
    "    return cleanup_tag(' '.join(tokens[start:end]))\n",
    "\n",
    "def get_complete_word(index: int, tokens: [str]) -> (str, int, bool):\n",
    "    current_word = tokens[index]\n",
    "    if not current_word.startswith('&'):\n",
    "        return current_word, index+1, False\n",
    "\n",
    "    word_parts = [current_word]\n",
    "    index += 1\n",
    "    if not current_word.endswith('!&'):\n",
    "        while index < len(tokens):\n",
    "            current_word = tokens[index]\n",
    "            word_parts.append(current_word)\n",
    "            if current_word.endswith('!&'):\n",
    "                index += 1\n",
    "                break\n",
    "            index += 1\n",
    "        else:\n",
    "            raise ValueError('No matching closing tag in tokens: \"%s\"' % tokens)\n",
    "    \n",
    "    word = cleanup_tag(' '.join(word_parts))\n",
    "    return word, index, True\n",
    "\n",
    "assert ('a', 1, False) == get_complete_word(0, ['a', 'b'])\n",
    "assert ('b', 2, False) == get_complete_word(1, ['a', 'b'])\n",
    "assert ('a', 1, True) == get_complete_word(0, ['&a!&', 'b']), get_complete_word(0, ['&a!&', 'b'])\n",
    "assert ('a b', 2, True) == get_complete_word(0, ['&a', 'b!&'])\n",
    "assert ('a b c', 3, True) == get_complete_word(0, ['&a', 'b', 'c!&'])\n",
    "\n",
    "def extract_contexts(text: str, window_size: int) -> [Context]:\n",
    "    result = []\n",
    "    \n",
    "    tokens = tokenize_text(text)\n",
    "    index = 0\n",
    "\n",
    "    while index < len(tokens):\n",
    "        word, new_index, is_tag = get_complete_word(index, tokens)\n",
    "        if not is_tag:\n",
    "            index = new_index\n",
    "            continue\n",
    "\n",
    "        left_context = get_span(tokens, index-5, index)\n",
    "        right_context = get_span(tokens, new_index, new_index+6)\n",
    "        left_word = get_span(tokens, index-1, index)\n",
    "        \n",
    "        context = Context(left_context, right_context, left_word, word)\n",
    "        result.append(context)\n",
    "        \n",
    "        index = new_index\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_contexts_dataframe(texts, window_size):\n",
    "    left = []\n",
    "    left_words = []\n",
    "    entities = []\n",
    "    right = []\n",
    "    for text in texts:\n",
    "        contexts = extract_contexts(text, window_size)\n",
    "        for context in contexts:\n",
    "            left_context = context.left_context\n",
    "            left.append(left_context)\n",
    "            left_word = context.left_word\n",
    "            left_words.append(left_word)\n",
    "            entity = context.word\n",
    "            entities.append(entity)\n",
    "            right_context = context.right_context\n",
    "            right.append(right_context)\n",
    "    dataframe = pd.DataFrame(np.column_stack([left, left_words, entities, right]),\n",
    "                            columns=['left_context', 'left_word', \n",
    "                                     'named_entity', 'right_context'])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dataframe(dataframe, column):\n",
    "    return dataframe.sort_values(by=[column], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "lectures_con = compile_contexts_dataframe(tagged_lectures, 5)\n",
    "lectures_con = sort_dataframe(lectures_con, 'left_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "postn_con = compile_contexts_dataframe(tagged_postn, 5)\n",
    "postn_con = sort_dataframe(postn_con, 'left_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_concat = tagged_postn + tagged_lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4616"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_con = compile_contexts_dataframe(texts_concat, 5)\n",
    "texts_con = sort_dataframe(texts_con, 'left_word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_con.to_csv('contexts_postn_lectures.tsv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
